# Optimization



### Bayesian optimzation

즉, 목적 함수(탐색대상함수)와 해당 하이퍼파라미터 쌍(pair)을 대상으로 Surrogate Model(대체 모델) 을 만들고, 평가를 통해 순차적으로 업데이트해 가면서 최적의 하이퍼파라미터 조합을 탐색하는 방법.

- **자세한 수행 과정**
  1. 입력값, 목적 함수 및 그 외 설정값들을 정의합니다.
     - 입력값 x : 여러가지 hyperparameter
     - 목적 함수 f(x) : 설정한 입력값을 적용하여 학습한 딥러닝 모델의 검증 데이터셋에 대한 성능 결과 수치(e.g. 정확도)
     - 입력값 의 탐색 대상 구간 : (a,b)
     - 맨 처음에 조사할 입력값-함숫값 점들의 갯수 : n
     - 맨 마지막 차례까지 조사할 입력값-함숫값 점들의 최대 갯수 : N
  2. 설정한 탐색 대상 구간 (a,b) 내에서 처음 n 개의 입력값들을 랜덤하게 샘플링하여 선택합니다.
  3. 선택한 n 개의 입력값 x1,x2,...,xn 을 각각 학습률 값으로 설정하여 딥러닝 모델을 학습한 뒤, 검증 데이터셋을 사용하여 학습이 완료된 모델의 성능 결과 수치를 계산합니다. 이들을 각각 함숫값 f(x1),f(x2),...,f(xn) 으로 간주합니다.
  4. 입력값-함숫값 점들의 모음 (x1,f(x1)),(x2,f(x2)),...,(xn,f(xn)) 에 대하여 Surrogate Model로 확률적 추정을 수행합니다.
  5. 조사된 입력값-함숫값 점들이 총 N 개에 도달할 때까지, 아래의 과정을 t=n,n+1,...,N−1 에 대하여 반복적으로 수행합니다.
     - 기존 입력값-함숫값 점들의 모음 (x1,f(x1)),(x2,f(x2)),...,(xt,f(xt)) 에 대한 Surrogate Model의 확률적 추정 결과를 바탕으로, 입력값 구간 (a,b) 내에서의 EI의 값을 계산하고, 그 값이 가장 큰 점을 다음 입력값 후보 xt+1 로 선정합니다.
     - 다음 입력값 후보 xt+1 를 학습률 값으로 설정하여 딥러닝 모델을 학습한 뒤, 검증 데이터셋을 사용하여 학습이 완료된 모델의 성능 결과 수치를 계산하고 이를 f(xt+1) 값으로 간주합니다.
     - 새로운 점 (xt+1,f(xt+1)) 을 기존 입력값-함숫값 점들의 모음에 추가하고, 갱신된 점들의 모음에 대하여 Surrogate Model로 확률적 추정을 다시 수행합니다.
  6. 총 N 개의 입력값-함숫값 점들에 대하여 확률적으로 추정된 목적 함수 결과물을 바탕으로, 평균 함수 μ(x)을 최대로 만드는 최적해를 최종 선택합니다. 추후 해당값을 학습률로 사용하여 딥러닝 모델을 학습하면, 일반화 성능이 극대화된 모델을 얻을 수 있습니다.

https://wooono.tistory.com/102

https://jihyun22.github.io/automl/Bayesian-Optimization/

